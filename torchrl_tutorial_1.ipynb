{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/rl/tutorials/coding_ppo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twsong/miniconda3/envs/torch-cert/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/twsong/miniconda3/envs/torch-cert/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (\n",
    "    Compose,\n",
    "    DoubleToFloat,\n",
    "    ObservationNorm,\n",
    "    StepCounter,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimisation steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Loc/Scale are already initialized: (tensor([  0.0783,  -0.0633,   0.0387, -11.8424,  -6.6971,   0.1065,  -0.0936,\n          0.0759,  -0.0000,  -0.0000,  -0.0000], device='cuda:0',\n       dtype=torch.float64), tensor([5.8735e+00, 3.0790e+00, 2.4482e+00, 1.2568e+01, 7.4187e+00, 6.0981e-01,\n        2.9863e-01, 2.5166e-01, 1.0000e+06, 1.0000e+06, 1.0000e+06],\n       device='cuda:0', dtype=torch.float64))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-cert/lib/python3.9/site-packages/torchrl/envs/transforms/transforms.py:2464\u001b[0m, in \u001b[0;36mObservationNorm.init_stats\u001b[0;34m(self, num_iter, reduce_dim, cat_dim, key, keep_dims)\u001b[0m\n\u001b[1;32m   2462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_dim must be part of or equal to reduce_dim.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized:\n\u001b[0;32m-> 2464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2465\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoc/Scale are already initialized: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2466\u001b[0m     )\n\u001b[1;32m   2468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2470\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform has multiple in_keys but no specific key was passed as an argument\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2471\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Loc/Scale are already initialized: (tensor([  0.0783,  -0.0633,   0.0387, -11.8424,  -6.6971,   0.1065,  -0.0936,\n          0.0759,  -0.0000,  -0.0000,  -0.0000], device='cuda:0',\n       dtype=torch.float64), tensor([5.8735e+00, 3.0790e+00, 2.4482e+00, 1.2568e+01, 7.4187e+00, 6.0981e-01,\n        2.9863e-01, 2.5166e-01, 1.0000e+06, 1.0000e+06, 1.0000e+06],\n       device='cuda:0', dtype=torch.float64))"
     ]
    }
   ],
   "source": [
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant shape: torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    observation: UnboundedContinuousTensorSpec(\n",
      "        shape=torch.Size([11]),\n",
      "        space=None,\n",
      "        device=cuda:0,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    step_count: BoundedTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.int64,\n",
      "        domain=continuous), device=cuda:0, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n",
      "input_spec: CompositeSpec(\n",
      "    full_state_spec: CompositeSpec(\n",
      "        step_count: BoundedTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.int64,\n",
      "            domain=continuous), device=cuda:0, shape=torch.Size([])),\n",
      "    full_action_spec: CompositeSpec(\n",
      "        action: BoundedTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cuda:0, shape=torch.Size([])), device=cuda:0, shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): BoundedTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PNDEnv' object has no attribute 'set_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheck_env_specs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-cert/lib/python3.9/site-packages/torchrl/envs/utils.py:448\u001b[0m, in \u001b[0;36mcheck_env_specs\u001b[0;34m(env, return_contiguous, check_dtype, seed)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 448\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_seed\u001b[49m(seed)\n\u001b[1;32m    450\u001b[0m fake_tensordict \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mfake_tensordict()\n\u001b[1;32m    451\u001b[0m real_tensordict \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrollout(\u001b[38;5;241m3\u001b[39m, return_contiguous\u001b[38;5;241m=\u001b[39mreturn_contiguous)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PNDEnv' object has no attribute 'set_seed'"
     ]
    }
   ],
   "source": [
    "check_env_specs(base_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
