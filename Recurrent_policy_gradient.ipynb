{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vYB_EG4Uj8MC"},"outputs":[],"source":["!pip install gymnasium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWL2A_5tjKdO"},"outputs":[],"source":["import sys\n","import os\n","from typing import Dict, List, Tuple\n","import gymnasium as gym\n","from gymnasium import Env\n","from gymnasium import spaces\n","from gymnasium.spaces import Discrete, Box, MultiBinary\n","import collections\n","import random\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from datetime import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.tensorboard import SummaryWriter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5JxL3gBjne4"},"outputs":[],"source":["# Hyperparameters\n","learning_rate = 0.0005\n","gamma         = 0.98\n","buffer_limit  = 50000\n","batch_size    = 32\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","POWERCOEFF = 0.1\n","AGECOEFF = 0.1\n","\n","class PNDEnv(Env):\n","    def __init__(self, **kwargs):\n","            \"\"\"\n","            Initialize the PNDEnv class.\n","\n","            Parameters:\n","            - n (int): The number of nodes in the environment.\n","            - density (float): The density of the environment.\n","            - max_epi (int): The maximum number of episodes.\n","            - model (str): The model to be used.\n","\n","            Returns:\n","            None\n","            \"\"\"\n","            super(PNDEnv, self).__init__()\n","            self.n = kwargs.get(\"n\", 10)\n","            self.density = kwargs.get(\"density\", 0.5)\n","            self.model = kwargs.get(\"model\", None)\n","            self.max_episode_length = kwargs.get(\"max_episode_length\", 2000)\n","\n","            # Actions we can take 0) transmit and 1) listen\n","            self.action_space = MultiBinary(self.n)\n","            # Observation space\n","            self.observation_space = spaces.Dict({\n","                \"current_age\": Box(low=0, high=1, shape=(self.n, 1)),\n","                \"prev_result\": MultiBinary([self.n, 1]),\n","                # 0: Listening, 1: Transmitting\n","            })\n","\n","    def get_obs(self):\n","        current_age = np.reshape(self._current_age, newshape=(self.n))\n","        prev_result = np.reshape(self._prev_result, newshape=(self.n))\n","        return np.concatenate([current_age, prev_result])\n","\n","    def get_info(self):\n","        print(\"Current Age, Prev Result\")\n","        for i in range(self.n):\n","            print(f\"Node {i}: {self._current_age[i]}, {self._prev_result[i]}\")\n","\n","    def reset(self, seed=None):\n","        super().reset(seed=seed)\n","        # State reset\n","        self._current_age = np.zeros(self.n)\n","        self._prev_result = np.zeros(self.n)\n","        self.adjacency_matrix = self.make_adjacency_matrix()  # Adjacency matrix\n","        self.where_packet_is_from = np.array([None]*self.n)\n","        self.episode_length = 300\n","\n","        observation = self.get_obs()\n","        return observation, None\n","\n","    def step(self, action: np.array):  # 여기 해야 함.\n","        # Check if the action is valid. Action length must be equal to the number of nodes and action must be 0 or 1.\n","\n","        assert len(action) == len(self._prev_result), \"Action length must be equal to the number of nodes.\"\n","        assert all([a in [0, 1] for a in action]), \"Action must be 0 or 1.\"\n","        self.where_packet_is_from = np.array([None]*self.n)\n","        self._prev_result = action\n","\n","        action_tiled = np.tile(action.reshape(-1, 1), (1, self.n))\n","        txrx_matrix = np.multiply(self.adjacency_matrix, action_tiled)\n","\n","        for i in np.where(action==1)[0]:\n","            txrx_matrix[:, i] = 0\n","\n","        collided_index = np.sum(txrx_matrix, axis=0)>1\n","        txrx_matrix[:, collided_index] = 0\n","\n","        # n_txtrial = np.count_nonzero(action)\n","        idx_success = np.where(np.sum(txrx_matrix, axis=1)!=0)[0]\n","        n_txtrial = len(idx_success)\n","\n","        self._current_age += 1/self.max_episode_length\n","        self._current_age = np.clip(self._current_age, 0, 1)\n","        self._current_age[idx_success] = 0\n","        self.episode_length -= 1\n","\n","        # reward = n_txtrial/self.max_episode_length - max(self._current_age) # 보낸 갯수만큼 보상을 준다.\n","        reward = n_txtrial - AGECOEFF*max(self._current_age) # 보낸 갯수만큼 보상을 준다.\n","\n","        done = (self.episode_length == 0)\n","        observation = self.get_obs()\n","\n","        return observation, reward, False, done, None\n","\n","\n","    def render(self):\n","        # Implement viz\n","        pass\n","\n","    def make_adjacency_matrix(self) -> np.ndarray:\n","        \"\"\"Make adjacency matrix of a clique network.\n","\n","        Args:\n","            n (int): Number of nodes.\n","            density (float): Density of the clique network.\n","\n","        Returns:\n","            np.ndarray: Adjacency matrix.\n","        \"\"\"\n","        if self.density < 0 or self.density > 1:\n","            raise ValueError(\"Density must be between 0 and 1.\")\n","\n","        n_edges = int(self.n * (self.n - 1) / 2 * self.density)\n","        adjacency_matrix = np.zeros((self.n, self.n))\n","\n","        if self.model == \"dumbbell\":\n","            adjacency_matrix[0, self.n-1] = 1\n","            adjacency_matrix[self.n-1, 0] = 1\n","            for i in range(1, self.n//2):\n","                adjacency_matrix[0, i] = 1\n","                adjacency_matrix[i, 0] = 1\n","            for i in range(self.n//2+1, self.n):\n","                adjacency_matrix[i-1, self.n-1] = 1\n","                adjacency_matrix[self.n-1, i-1] = 1\n","        elif self.model == \"linear\":\n","            for i in range(1, self.n):\n","                adjacency_matrix[i-1, i] = 1\n","                adjacency_matrix[i, i-1] = 1\n","        else:\n","            for i in range(1, self.n):\n","                adjacency_matrix[i-1, i] = 1\n","                adjacency_matrix[i, i-1] = 1\n","                n_edges -= 1\n","            # If the density of the current adjacency matrix is over density, return it.\n","            if n_edges <= 0:\n","                return adjacency_matrix\n","            else:\n","                arr = [1]*n_edges + [0]*((self.n-1)*(self.n-2)//2 - n_edges)\n","                np.random.shuffle(arr)\n","                for i in range(0, self.n):\n","                    for j in range(i+2, self.n):\n","                        adjacency_matrix[i, j] = arr.pop()\n","                        adjacency_matrix[j, i] = adjacency_matrix[i, j]\n","        return adjacency_matrix\n","\n","    def show_adjacency_matrix(self):\n","        print(self.adjacency_matrix)\n","\n","    def save_graph_with_labels(self, path):\n","        rows, cols = np.where(self.adjacency_matrix == 1)\n","        edges = zip(rows.tolist(), cols.tolist())\n","        G = nx.Graph()\n","        G.add_edges_from(edges)\n","        pos = nx.kamada_kawai_layout(G)\n","        nx.draw_networkx(G, pos=pos, with_labels=True)\n","        plt.savefig(path + '/adj_graph.png')\n","\n","    def get_current_age(self):\n","        return self._current_age\n","\n","\n","class Policy(nn.Module):\n","    def __init__(self, state_space=2, action_space=2):\n","        super(Policy, self).__init__()\n","        self.data = []\n","        self.state_space = state_space\n","        self.hidden_space = 4\n","        self.action_space = action_space\n","        self.linear1 = nn.Linear(self.state_space, self.hidden_space)\n","        self.lstm = nn.LSTM(self.hidden_space, self.hidden_space)\n","        self.linear2 = nn.Linear(self.hidden_space, self.action_space)\n","\n","    def forward(self, x, h, c):\n","        x = F.relu(self.linear1(x))\n","        x, (new_h, new_c) = self.lstm(x, (h, c))\n","        x = F.softmax(self.linear2(x), dim=2)\n","        return x, new_h, new_c\n","\n","    def put_data(self, transition):\n","        self.data.append(transition)\n","\n","    def sample_action(self, obs, h, c):\n","        output = self.forward(obs, h, c)\n","        # Select action with respect to the action probabilities\n","        action = torch.squeeze(output[0]).multinomial(num_samples=1)\n","        return action.item(), output[1], output[2]\n","\n","    def init_hidden_state(self):\n","        return torch.zeros(1, 1, self.hidden_space, device=device), torch.zeros(1, 1, self.hidden_space, device=device)\n","\n","def train(pi, optimizer):\n","    R = 0\n","    policy_loss = []\n","    optimizer.zero_grad()\n","    for r, prob in pi.data[::-1]:\n","        R = r + gamma * R\n","        loss = -torch.log(prob) * R # Negative score function x reward\n","        policy_loss.append(loss)\n","    sum(policy_loss).backward()\n","    optimizer.step()\n","    pi.data = []\n","\n","\n","# Set parameters\n","batch_size = 8\n","learning_rate = 1e-3\n","buffer_len = int(100000)\n","min_epi_num = 10 # Start moment to train the Q network\n","episodes = 5000\n","target_update_period = 10\n","eps_start = 0.1\n","eps_end = 0.001\n","eps_decay = 0.995\n","tau = 1e-2\n","\n","# DRQN param\n","random_update = True    # If you want to do random update instead of sequential update\n","lookup_step = 20        # If you want to do random update instead of sequential update\n","\n","# Number of envs param\n","n_nodes = 10\n","n_agents = 10\n","density = 1\n","max_step = 300\n","model = None\n","\n","# Set gym environment\n","env_params = {\n","    \"n\": n_nodes,\n","    \"density\": density,\n","    \"max_episode_length\": max_step,\n","    \"model\": model\n","    }\n","if model == None:\n","    env_params_str = f\"n{n_nodes}_density{density}_max_episode_length{max_step}\"\n","else:\n","    env_params_str = f\"n{n_nodes}_model{model}_max_episode_length{max_step}\"\n","\n","env = PNDEnv(**env_params)\n","env.reset()\n","\n","output_path = 'outputs/R2inforce_'+env_params_str\n","writer = SummaryWriter(filename_suffix=env_params_str)\n","if not os.path.exists(output_path):\n","    os.makedirs(output_path)\n","env.save_graph_with_labels(output_path)\n","\n","# Create Policy functions\n","n_states = 2\n","n_actions = 2\n","\n","pi_cum = [Policy(state_space=n_states, action_space=n_actions).to(device) for _ in range(n_agents)]\n","\n","# Set optimizer\n","optimizer_cum = [optim.Adam(pi_cum[i].parameters(), lr=learning_rate) for i in range(n_agents)]\n","\n","epsilon = eps_start\n","\n","df = pd.DataFrame(columns=['episode', 'time'] + [f'action_{i}' for i in range(n_agents)] + [f'age_{i}' for i in range(n_agents)])\n","appended_df = []\n","\n","for i_epi in tqdm(range(episodes), desc=\"Episodes\", position=0, leave=True):\n","    s, _ = env.reset()\n","    score = 0.0\n","    obs_cum = [s[np.array([x, x+n_agents])] for x in range(n_agents)]\n","    h_cum, c_cum = zip(*[pi_cum[i].init_hidden_state() for i in range(n_agents)])\n","    done = False\n","\n","    for t in tqdm(range(max_step), desc=\"   Steps\", position=1, leave=False):\n","        prob_cum = [pi_cum[i](torch.from_numpy(obs_cum[i]).float().unsqueeze(0).unsqueeze(0).to(device), h_cum[i].to(device), c_cum[i].to(device))[0] for i in range(n_agents)]\n","        a_cum, h_cum, c_cum = zip(*[pi_cum[i].sample_action(torch.from_numpy(obs_cum[i]).float().unsqueeze(0).unsqueeze(0).to(device), h_cum[i].to(device), c_cum[i].to(device)) for i in range(n_agents)])\n","        a_cum = np.array(a_cum)\n","        s_prime, r, done, _, info = env.step(a_cum)\n","        done_mask = 0.0 if done else 1.0\n","        for i in range(n_agents):\n","            a = a_cum[i]\n","            pi_cum[i].put_data((r, prob_cum[i][0, 0, a]))\n","        obs_cum = [s_prime[np.array([x, x+n_agents])] for x in range(n_agents)]\n","        score += r\n","\n","        df_currepoch = pd.DataFrame(data=[[i_epi, t, *a_cum, *env.get_current_age()]],\n","                                    columns=['episode', 'time'] + [f'action_{i}' for i in range(n_agents)] + [f'age_{i}' for i in range(n_agents)])\n","        appended_df.append(df_currepoch)\n","\n","        if done:\n","            break\n","\n","    for pi, optimizer in zip(pi_cum, optimizer_cum):\n","        train(pi, optimizer)\n","\n","    print(f\"n_episode: {i_epi}/{episodes}, score: {score}\")\n","    writer.add_scalar('Rewards per episodes', score, i_epi)\n","    score = 0\n","\n","for i in range(n_agents):\n","    torch.save(pi_cum[i].state_dict(), output_path + f\"/R2_cum_{i}.pth\")\n","\n","df = pd.concat(appended_df, ignore_index=True)\n","current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n","df.to_csv(output_path + f\"/log_{current_time}.csv\", index=False)\n","writer.close()\n","env.close()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOmBl99jVaQXs5stDf0OqGJ","gpuType":"V100","machine_shape":"hm","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}
